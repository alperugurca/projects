{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2333429,"sourceType":"datasetVersion","datasetId":1408532}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Animal Classification with CNN  \n\n- Overview:\nThis notebook builds and evaluates a CNN model to classify 10 animal species.\n\nThe model is tested on three sets:\n\n* Original Test Set – Standard images.  \n* Manipulated Test Set – Images with brightness/contrast changes.  \n* Color-Corrected Test Set – Images processed with the Gray World Algorithm for color constancy.  \n\nKey Steps:\n* Data Preprocessing: Resize, normalize, and split data.  \n* Model Training: Train a basic CNN model.  \n* Evaluation: Compare performance across the three test sets.  \n\nDataset: https://www.kaggle.com/datasets/rrebirrth/animals-with-attributes-2/data","metadata":{"execution":{"iopub.status.busy":"2024-12-21T16:47:52.357915Z","iopub.execute_input":"2024-12-21T16:47:52.358198Z","iopub.status.idle":"2024-12-21T16:47:52.363679Z","shell.execute_reply.started":"2024-12-21T16:47:52.358175Z","shell.execute_reply":"2024-12-21T16:47:52.362529Z"}}},{"cell_type":"markdown","source":"# 1. Read and Prepare Dataset","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import to_categorical\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T17:24:27.847070Z","iopub.execute_input":"2024-12-21T17:24:27.847432Z","iopub.status.idle":"2024-12-21T17:24:31.173143Z","shell.execute_reply.started":"2024-12-21T17:24:27.847393Z","shell.execute_reply":"2024-12-21T17:24:31.172473Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"source = \"/kaggle/input/animals-with-attributes-2/Animals_with_Attributes2/JPEGImages\"  \ntarget = \"/kaggle/working/FilteredImages\"\n\nclasses = [\"collie\", \"dolphin\", \"elephant\", \"fox\", \"moose\", \"rabbit\", \"sheep\", \"squirrel\", \"giant+panda\", \"polar+bear\"]\nimages_per_class = 650\n\n# Create the target directory\nos.makedirs(target, exist_ok=True)\n\n# Loop through each class and copy the images\nfor class_name in classes:\n    class_path = os.path.join(source, class_name)\n    target_path = os.path.join(target, class_name)\n    \n    if not os.path.exists(class_path):\n        print(f\"Source class path {class_path} does not exist. Skipping...\")\n        continue\n    \n    # Create a subdirectory for each class in the target directory\n    os.makedirs(target_path, exist_ok=True)\n    \n    print(f\"Processing class: {class_name}\")\n    image_count = 0\n    available_files = os.listdir(class_path)\n    \n    # Adjust images_per_class to the number of available images\n    total_images = len(available_files)\n    images_to_copy = min(images_per_class, total_images)\n    print(f\"Found {total_images} images. Attempting to copy {images_to_copy} images.\")\n    \n    for file_name in available_files:\n        if image_count >= images_to_copy:\n            break\n        \n        full_file_name = os.path.join(class_path, file_name)\n        if os.path.isfile(full_file_name):\n            img = cv2.imread(full_file_name)\n            \n            if img is not None:  # Ensure the image is read properly\n                cv2.imwrite(os.path.join(target_path, file_name), img)\n                image_count += 1\n            else:\n                print(f\"Warning: Unable to read image {full_file_name}\")\n    \n    print(f\"Completed {image_count}/{images_to_copy} images for class {class_name}\")","metadata":{"execution":{"iopub.status.busy":"2024-12-21T17:24:31.173917Z","iopub.execute_input":"2024-12-21T17:24:31.174383Z","iopub.status.idle":"2024-12-21T17:25:58.377892Z","shell.execute_reply.started":"2024-12-21T17:24:31.174358Z","shell.execute_reply":"2024-12-21T17:25:58.376973Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_and_process_images(data_dir, image_size=(128, 128)):\n    images = []\n    labels = []\n    for class_name in os.listdir(data_dir):\n        class_path = os.path.join(data_dir, class_name)\n        if os.path.isdir(class_path):  # Ensure it's a directory\n            for file_name in os.listdir(class_path):\n                file_path = os.path.join(class_path, file_name)\n                try:\n                    img = cv2.imread(file_path)\n                    if img is not None:  # Ensure the image is loaded\n                        img_resized = cv2.resize(img, image_size)\n                        img_normalized = img_resized / 255.0  # Normalize pixel values\n                        images.append(img_normalized)\n                        labels.append(class_name)\n                except Exception as e:\n                    print(f\"Error processing file {file_path}: {e}\")\n    return np.array(images), np.array(labels)\n\n# Define your data directory\ndata_dir = target  # Use 'target' as defined earlier\n\n# Load and process images\nX, y = load_and_process_images(data_dir)\n\n# Print dataset size and shape\nprint(f\"Dataset size: {len(X)} images\")\nprint(f\"Image shape: {X[0].shape if len(X) > 0 else 'No images loaded'}\")\nprint(f\"Labels size: {len(y)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T17:25:58.378858Z","iopub.execute_input":"2024-12-21T17:25:58.379191Z","iopub.status.idle":"2024-12-21T17:26:37.547502Z","shell.execute_reply.started":"2024-12-21T17:25:58.379157Z","shell.execute_reply":"2024-12-21T17:26:37.546576Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Encode the labels\nencoder = LabelEncoder()\ny_encoded = encoder.fit_transform(y)  # Convert string labels to integers\ny_categorical = to_categorical(y_encoded)  # Convert to one-hot encoding\n\n# Check label encoding\nprint(f\"Classes: {encoder.classes_}\")\nprint(f\"Encoded labels: {y_encoded[:10]}\")\nprint(f\"One-hot encoded labels shape: {y_categorical.shape}\")\n\n# Split the data (70% training, 30% testing)\nX_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.3, random_state=42)\n\n# Print dataset shapes\nprint(f\"Training data shape: {X_train.shape}, Test data shape: {X_test.shape}\")\nprint(f\"Training labels shape: {y_train.shape}, Test labels shape: {y_test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T17:26:37.548489Z","iopub.execute_input":"2024-12-21T17:26:37.548771Z","iopub.status.idle":"2024-12-21T17:26:38.276951Z","shell.execute_reply.started":"2024-12-21T17:26:37.548747Z","shell.execute_reply":"2024-12-21T17:26:38.276166Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"datagen = ImageDataGenerator(\n    rotation_range=15,  # Rotate images by up to 15 degrees\n    width_shift_range=0.1,  # Shift images horizontally by up to 10% of width\n    height_shift_range=0.1,  # Shift images vertically by up to 10% of height\n    shear_range=0.1,  # Shear images by up to 10%\n    zoom_range=0.1,  # Zoom images in/out by up to 10%\n    horizontal_flip=True,  # Randomly flip images horizontally\n    fill_mode='nearest'  # Fill any missing pixels using the nearest pixel\n)\ndatagen.fit(X_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T17:26:38.278888Z","iopub.execute_input":"2024-12-21T17:26:38.279128Z","iopub.status.idle":"2024-12-21T17:26:38.920080Z","shell.execute_reply.started":"2024-12-21T17:26:38.279107Z","shell.execute_reply":"2024-12-21T17:26:38.919411Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. CNN Model Build","metadata":{}},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))  # Pooling layer\n\n# Convolutional layer 2\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))  # Pooling layer\n\n# Flatten the feature maps\nmodel.add(Flatten())\n\n# Output layer\nmodel.add(Dense(10, activation='softmax'))  # 10 classes\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=0.001),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T17:26:38.921286Z","iopub.execute_input":"2024-12-21T17:26:38.921594Z","iopub.status.idle":"2024-12-21T17:26:39.204716Z","shell.execute_reply.started":"2024-12-21T17:26:38.921564Z","shell.execute_reply":"2024-12-21T17:26:39.203953Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. CNN Model Test","metadata":{}},{"cell_type":"code","source":"history = model.fit(\n    X_train, y_train,\n    validation_split=0.2,  # Use 20% of the training data for validation\n    epochs=10,  # Train for 5 epochs\n    batch_size=32,  # Use batches of 32 images\n    verbose=1\n)\n\ntest_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\nprint(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T17:26:39.205569Z","iopub.execute_input":"2024-12-21T17:26:39.205832Z","iopub.status.idle":"2024-12-21T17:27:00.439056Z","shell.execute_reply.started":"2024-12-21T17:26:39.205810Z","shell.execute_reply":"2024-12-21T17:27:00.438307Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. Manipulation of Images with Different Lights and Testing with Manipulated Test Set","metadata":{}},{"cell_type":"code","source":"# Simple image manipulation (e.g., increasing contrast)\ndef manipulate_images(images):\n    return np.array([cv2.convertScaleAbs(img, alpha=2.0, beta=0) for img in images])  # Contrast adjustment\n\n# Apply manipulation\nX_test_manipulated = manipulate_images(X_test)\n\n# Evaluate the model on manipulated images\nmanipulated_loss, manipulated_accuracy = model.evaluate(X_test_manipulated, y_test)\nprint(f\"Accuracy on Manipulated Test Images (with contrast): {manipulated_accuracy * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T17:27:00.439898Z","iopub.execute_input":"2024-12-21T17:27:00.440225Z","iopub.status.idle":"2024-12-21T17:27:01.492336Z","shell.execute_reply.started":"2024-12-21T17:27:00.440166Z","shell.execute_reply":"2024-12-21T17:27:01.491614Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. Applying the Color Constancy Algorithm to the Manipulated Test Set and Testing with the Color Constancy Test Set","metadata":{}},{"cell_type":"code","source":"# Gray World algorithm for color constancy\ndef get_wb_images(images):\n    wb_images = []\n    \n    for img in images:\n        # Convert the image to RGB if it's not already\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        # Calculate the mean color of the image\n        mean_r = np.mean(img_rgb[:,:,0])\n        mean_g = np.mean(img_rgb[:,:,1])\n        mean_b = np.mean(img_rgb[:,:,2])\n        \n        # Calculate the average color\n        avg = (mean_r + mean_g + mean_b) / 3\n        \n        # Adjust each channel to balance the colors\n        img_rgb[:,:,0] = img_rgb[:,:,0] * (avg / mean_r)\n        img_rgb[:,:,1] = img_rgb[:,:,1] * (avg / mean_g)\n        img_rgb[:,:,2] = img_rgb[:,:,2] * (avg / mean_b)\n        \n        # Clip the values to keep them in the 0-255 range\n        img_rgb = np.clip(img_rgb, 0, 255)\n        \n        # Convert back to BGR (if needed) and store\n        wb_images.append(cv2.cvtColor(img_rgb.astype(np.uint8), cv2.COLOR_RGB2BGR))\n    \n    return np.array(wb_images)\n\n# Apply the Gray World algorithm to the manipulated test images\nX_test_wb = get_wb_images(X_test_manipulated)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T17:27:01.493068Z","iopub.execute_input":"2024-12-21T17:27:01.493384Z","iopub.status.idle":"2024-12-21T17:27:01.895013Z","shell.execute_reply.started":"2024-12-21T17:27:01.493351Z","shell.execute_reply":"2024-12-21T17:27:01.894293Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate the model on the color-corrected test set\nwb_loss, wb_accuracy = model.evaluate(X_test_wb, y_test)\n\n# Print the accuracy of the model on the color-constant corrected test set\nprint(f\"Accuracy on Color-Corrected Test Images: {wb_accuracy * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T17:27:01.895957Z","iopub.execute_input":"2024-12-21T17:27:01.896310Z","iopub.status.idle":"2024-12-21T17:27:02.206882Z","shell.execute_reply.started":"2024-12-21T17:27:01.896275Z","shell.execute_reply":"2024-12-21T17:27:02.206145Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6. Comparing and Reporting the Success of Different Test Sets","metadata":{}},{"cell_type":"code","source":"# Evaluate the model on the original test set\noriginal_loss, original_accuracy = model.evaluate(X_test, y_test)\nprint(f\"Accuracy on Original Test Set: {original_accuracy * 100:.2f}%\")\n\n# Evaluate the model on the manipulated test set\nmanipulated_loss, manipulated_accuracy = model.evaluate(X_test_manipulated, y_test)\nprint(f\"Accuracy on Manipulated Test Set: {manipulated_accuracy * 100:.2f}%\")\n\n# Evaluate the model on the color-corrected (Gray World) test set\nwb_loss, wb_accuracy = model.evaluate(X_test_wb, y_test)\nprint(f\"Accuracy on Color-Corrected Test Set: {wb_accuracy * 100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T17:27:02.207611Z","iopub.execute_input":"2024-12-21T17:27:02.207848Z","iopub.status.idle":"2024-12-21T17:27:04.177119Z","shell.execute_reply.started":"2024-12-21T17:27:02.207826Z","shell.execute_reply":"2024-12-21T17:27:04.176424Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Accuracy on Original Test Set: 52.21%\n\n- Accuracy on Manipulated Test Set: 46.31%\n\n- Accuracy on Color-Corrected Test Set: 8.15%\n\nThe model performs well on the original test set, but its accuracy drops significantly when exposed to manipulated or color-corrected images.  \nThis suggests that the model needs further improvement to handle real-world lighting variations.  \nPossible solutions include data augmentation, transfer learning, and experimenting with advanced color constancy algorithms.","metadata":{}}]}