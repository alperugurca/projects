{"cells":[{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2024-12-21T16:47:52.358198Z","iopub.status.busy":"2024-12-21T16:47:52.357915Z","iopub.status.idle":"2024-12-21T16:47:52.363679Z","shell.execute_reply":"2024-12-21T16:47:52.362529Z","shell.execute_reply.started":"2024-12-21T16:47:52.358175Z"}},"source":["Animal Classification with CNN  \n","\n","- Overview:\n","This notebook builds and evaluates a CNN model to classify 10 animal species.\n","\n","The model is tested on three sets:\n","\n","* Original Test Set – Standard images.  \n","* Manipulated Test Set – Images with brightness/contrast changes.  \n","* Color-Corrected Test Set – Images processed with the Gray World Algorithm for color constancy.  \n","\n","Key Steps:\n","* Data Preprocessing: Resize, normalize, and split data.  \n","* Model Training: Train a basic CNN model.  \n","* Evaluation: Compare performance across the three test sets.  \n","\n","Dataset: https://www.kaggle.com/datasets/rrebirrth/animals-with-attributes-2/data\n","\n","Kaggle: https://www.kaggle.com/code/alperugurca/animals-with-attributes-2"]},{"cell_type":"markdown","metadata":{},"source":["# 1. Read and Prepare Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-21T17:24:27.847432Z","iopub.status.busy":"2024-12-21T17:24:27.847070Z","iopub.status.idle":"2024-12-21T17:24:31.173143Z","shell.execute_reply":"2024-12-21T17:24:31.172473Z","shell.execute_reply.started":"2024-12-21T17:24:27.847393Z"},"trusted":true},"outputs":[],"source":["import os\n","import numpy as np\n","import cv2\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from sklearn.preprocessing import LabelEncoder\n","from tensorflow.keras.utils import to_categorical\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-21T17:24:31.174383Z","iopub.status.busy":"2024-12-21T17:24:31.173917Z","iopub.status.idle":"2024-12-21T17:25:58.377892Z","shell.execute_reply":"2024-12-21T17:25:58.376973Z","shell.execute_reply.started":"2024-12-21T17:24:31.174358Z"},"trusted":true},"outputs":[],"source":["source = \"/kaggle/input/animals-with-attributes-2/Animals_with_Attributes2/JPEGImages\"  \n","target = \"/kaggle/working/FilteredImages\"\n","\n","classes = [\"collie\", \"dolphin\", \"elephant\", \"fox\", \"moose\", \"rabbit\", \"sheep\", \"squirrel\", \"giant+panda\", \"polar+bear\"]\n","images_per_class = 650\n","\n","# Create the target directory\n","os.makedirs(target, exist_ok=True)\n","\n","# Loop through each class and copy the images\n","for class_name in classes:\n","    class_path = os.path.join(source, class_name)\n","    target_path = os.path.join(target, class_name)\n","    \n","    if not os.path.exists(class_path):\n","        print(f\"Source class path {class_path} does not exist. Skipping...\")\n","        continue\n","    \n","    # Create a subdirectory for each class in the target directory\n","    os.makedirs(target_path, exist_ok=True)\n","    \n","    print(f\"Processing class: {class_name}\")\n","    image_count = 0\n","    available_files = os.listdir(class_path)\n","    \n","    # Adjust images_per_class to the number of available images\n","    total_images = len(available_files)\n","    images_to_copy = min(images_per_class, total_images)\n","    print(f\"Found {total_images} images. Attempting to copy {images_to_copy} images.\")\n","    \n","    for file_name in available_files:\n","        if image_count >= images_to_copy:\n","            break\n","        \n","        full_file_name = os.path.join(class_path, file_name)\n","        if os.path.isfile(full_file_name):\n","            img = cv2.imread(full_file_name)\n","            \n","            if img is not None:  # Ensure the image is read properly\n","                cv2.imwrite(os.path.join(target_path, file_name), img)\n","                image_count += 1\n","            else:\n","                print(f\"Warning: Unable to read image {full_file_name}\")\n","    \n","    print(f\"Completed {image_count}/{images_to_copy} images for class {class_name}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-21T17:25:58.379191Z","iopub.status.busy":"2024-12-21T17:25:58.378858Z","iopub.status.idle":"2024-12-21T17:26:37.547502Z","shell.execute_reply":"2024-12-21T17:26:37.546576Z","shell.execute_reply.started":"2024-12-21T17:25:58.379157Z"},"trusted":true},"outputs":[],"source":["def load_and_process_images(data_dir, image_size=(128, 128)):\n","    images = []\n","    labels = []\n","    for class_name in os.listdir(data_dir):\n","        class_path = os.path.join(data_dir, class_name)\n","        if os.path.isdir(class_path):  # Ensure it's a directory\n","            for file_name in os.listdir(class_path):\n","                file_path = os.path.join(class_path, file_name)\n","                try:\n","                    img = cv2.imread(file_path)\n","                    if img is not None:  # Ensure the image is loaded\n","                        img_resized = cv2.resize(img, image_size)\n","                        img_normalized = img_resized / 255.0  # Normalize pixel values\n","                        images.append(img_normalized)\n","                        labels.append(class_name)\n","                except Exception as e:\n","                    print(f\"Error processing file {file_path}: {e}\")\n","    return np.array(images), np.array(labels)\n","\n","# Define your data directory\n","data_dir = target  # Use 'target' as defined earlier\n","\n","# Load and process images\n","X, y = load_and_process_images(data_dir)\n","\n","# Print dataset size and shape\n","print(f\"Dataset size: {len(X)} images\")\n","print(f\"Image shape: {X[0].shape if len(X) > 0 else 'No images loaded'}\")\n","print(f\"Labels size: {len(y)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-21T17:26:37.548771Z","iopub.status.busy":"2024-12-21T17:26:37.548489Z","iopub.status.idle":"2024-12-21T17:26:38.276951Z","shell.execute_reply":"2024-12-21T17:26:38.276166Z","shell.execute_reply.started":"2024-12-21T17:26:37.548747Z"},"trusted":true},"outputs":[],"source":["# Encode the labels\n","encoder = LabelEncoder()\n","y_encoded = encoder.fit_transform(y)  # Convert string labels to integers\n","y_categorical = to_categorical(y_encoded)  # Convert to one-hot encoding\n","\n","# Check label encoding\n","print(f\"Classes: {encoder.classes_}\")\n","print(f\"Encoded labels: {y_encoded[:10]}\")\n","print(f\"One-hot encoded labels shape: {y_categorical.shape}\")\n","\n","# Split the data (70% training, 30% testing)\n","X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.3, random_state=42)\n","\n","# Print dataset shapes\n","print(f\"Training data shape: {X_train.shape}, Test data shape: {X_test.shape}\")\n","print(f\"Training labels shape: {y_train.shape}, Test labels shape: {y_test.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-21T17:26:38.279128Z","iopub.status.busy":"2024-12-21T17:26:38.278888Z","iopub.status.idle":"2024-12-21T17:26:38.920080Z","shell.execute_reply":"2024-12-21T17:26:38.919411Z","shell.execute_reply.started":"2024-12-21T17:26:38.279107Z"},"trusted":true},"outputs":[],"source":["datagen = ImageDataGenerator(\n","    rotation_range=15,  # Rotate images by up to 15 degrees\n","    width_shift_range=0.1,  # Shift images horizontally by up to 10% of width\n","    height_shift_range=0.1,  # Shift images vertically by up to 10% of height\n","    shear_range=0.1,  # Shear images by up to 10%\n","    zoom_range=0.1,  # Zoom images in/out by up to 10%\n","    horizontal_flip=True,  # Randomly flip images horizontally\n","    fill_mode='nearest'  # Fill any missing pixels using the nearest pixel\n",")\n","datagen.fit(X_train)"]},{"cell_type":"markdown","metadata":{},"source":["# 2. CNN Model Build"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-21T17:26:38.921594Z","iopub.status.busy":"2024-12-21T17:26:38.921286Z","iopub.status.idle":"2024-12-21T17:26:39.204716Z","shell.execute_reply":"2024-12-21T17:26:39.203953Z","shell.execute_reply.started":"2024-12-21T17:26:38.921564Z"},"trusted":true},"outputs":[],"source":["model = Sequential()\n","\n","model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))\n","model.add(MaxPooling2D(pool_size=(2, 2)))  # Pooling layer\n","\n","# Convolutional layer 2\n","model.add(Conv2D(64, (3, 3), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))  # Pooling layer\n","\n","# Flatten the feature maps\n","model.add(Flatten())\n","\n","# Output layer\n","model.add(Dense(10, activation='softmax'))  # 10 classes\n","\n","# Compile the model\n","model.compile(optimizer=Adam(learning_rate=0.001),\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["# 3. CNN Model Test"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-21T17:26:39.205832Z","iopub.status.busy":"2024-12-21T17:26:39.205569Z","iopub.status.idle":"2024-12-21T17:27:00.439056Z","shell.execute_reply":"2024-12-21T17:27:00.438307Z","shell.execute_reply.started":"2024-12-21T17:26:39.205810Z"},"trusted":true},"outputs":[],"source":["history = model.fit(\n","    X_train, y_train,\n","    validation_split=0.2,  # Use 20% of the training data for validation\n","    epochs=10,  # Train for 5 epochs\n","    batch_size=32,  # Use batches of 32 images\n","    verbose=1\n",")\n","\n","test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n","print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# 4. Manipulation of Images with Different Lights and Testing with Manipulated Test Set"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-21T17:27:00.440225Z","iopub.status.busy":"2024-12-21T17:27:00.439898Z","iopub.status.idle":"2024-12-21T17:27:01.492336Z","shell.execute_reply":"2024-12-21T17:27:01.491614Z","shell.execute_reply.started":"2024-12-21T17:27:00.440166Z"},"trusted":true},"outputs":[],"source":["# Simple image manipulation (e.g., increasing contrast)\n","def manipulate_images(images):\n","    return np.array([cv2.convertScaleAbs(img, alpha=2.0, beta=0) for img in images])  # Contrast adjustment\n","\n","# Apply manipulation\n","X_test_manipulated = manipulate_images(X_test)\n","\n","# Evaluate the model on manipulated images\n","manipulated_loss, manipulated_accuracy = model.evaluate(X_test_manipulated, y_test)\n","print(f\"Accuracy on Manipulated Test Images (with contrast): {manipulated_accuracy * 100:.2f}%\")"]},{"cell_type":"markdown","metadata":{},"source":["# 5. Applying the Color Constancy Algorithm to the Manipulated Test Set and Testing with the Color Constancy Test Set"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-21T17:27:01.493384Z","iopub.status.busy":"2024-12-21T17:27:01.493068Z","iopub.status.idle":"2024-12-21T17:27:01.895013Z","shell.execute_reply":"2024-12-21T17:27:01.894293Z","shell.execute_reply.started":"2024-12-21T17:27:01.493351Z"},"trusted":true},"outputs":[],"source":["# Gray World algorithm for color constancy\n","def get_wb_images(images):\n","    wb_images = []\n","    \n","    for img in images:\n","        # Convert the image to RGB if it's not already\n","        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        \n","        # Calculate the mean color of the image\n","        mean_r = np.mean(img_rgb[:,:,0])\n","        mean_g = np.mean(img_rgb[:,:,1])\n","        mean_b = np.mean(img_rgb[:,:,2])\n","        \n","        # Calculate the average color\n","        avg = (mean_r + mean_g + mean_b) / 3\n","        \n","        # Adjust each channel to balance the colors\n","        img_rgb[:,:,0] = img_rgb[:,:,0] * (avg / mean_r)\n","        img_rgb[:,:,1] = img_rgb[:,:,1] * (avg / mean_g)\n","        img_rgb[:,:,2] = img_rgb[:,:,2] * (avg / mean_b)\n","        \n","        # Clip the values to keep them in the 0-255 range\n","        img_rgb = np.clip(img_rgb, 0, 255)\n","        \n","        # Convert back to BGR (if needed) and store\n","        wb_images.append(cv2.cvtColor(img_rgb.astype(np.uint8), cv2.COLOR_RGB2BGR))\n","    \n","    return np.array(wb_images)\n","\n","# Apply the Gray World algorithm to the manipulated test images\n","X_test_wb = get_wb_images(X_test_manipulated)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-21T17:27:01.896310Z","iopub.status.busy":"2024-12-21T17:27:01.895957Z","iopub.status.idle":"2024-12-21T17:27:02.206882Z","shell.execute_reply":"2024-12-21T17:27:02.206145Z","shell.execute_reply.started":"2024-12-21T17:27:01.896275Z"},"trusted":true},"outputs":[],"source":["# Evaluate the model on the color-corrected test set\n","wb_loss, wb_accuracy = model.evaluate(X_test_wb, y_test)\n","\n","# Print the accuracy of the model on the color-constant corrected test set\n","print(f\"Accuracy on Color-Corrected Test Images: {wb_accuracy * 100:.2f}%\")"]},{"cell_type":"markdown","metadata":{},"source":["# 6. Comparing and Reporting the Success of Different Test Sets"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-21T17:27:02.207848Z","iopub.status.busy":"2024-12-21T17:27:02.207611Z","iopub.status.idle":"2024-12-21T17:27:04.177119Z","shell.execute_reply":"2024-12-21T17:27:04.176424Z","shell.execute_reply.started":"2024-12-21T17:27:02.207826Z"},"trusted":true},"outputs":[],"source":["# Evaluate the model on the original test set\n","original_loss, original_accuracy = model.evaluate(X_test, y_test)\n","print(f\"Accuracy on Original Test Set: {original_accuracy * 100:.2f}%\")\n","\n","# Evaluate the model on the manipulated test set\n","manipulated_loss, manipulated_accuracy = model.evaluate(X_test_manipulated, y_test)\n","print(f\"Accuracy on Manipulated Test Set: {manipulated_accuracy * 100:.2f}%\")\n","\n","# Evaluate the model on the color-corrected (Gray World) test set\n","wb_loss, wb_accuracy = model.evaluate(X_test_wb, y_test)\n","print(f\"Accuracy on Color-Corrected Test Set: {wb_accuracy * 100:.2f}%\")\n"]},{"cell_type":"markdown","metadata":{},"source":["- Accuracy on Original Test Set: 52.21%\n","\n","- Accuracy on Manipulated Test Set: 46.31%\n","\n","- Accuracy on Color-Corrected Test Set: 8.15%\n","\n","The model performs well on the original test set, but its accuracy drops significantly when exposed to manipulated or color-corrected images.  \n","This suggests that the model needs further improvement to handle real-world lighting variations.  \n","Possible solutions include data augmentation, transfer learning, and experimenting with advanced color constancy algorithms."]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":1408532,"sourceId":2333429,"sourceType":"datasetVersion"}],"dockerImageVersionId":30823,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
