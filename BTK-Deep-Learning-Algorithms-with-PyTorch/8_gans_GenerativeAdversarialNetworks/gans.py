"""
image generation: MNIST dataset
"""
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import torchvision.utils as utils
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np

import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# %% prepare dataset
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

batch_size = 128  # mini-batch size
image_size = 28*28  # image size

transform = transforms.Compose([
    transforms.ToTensor(),  # convert images to tensor
    transforms.Normalize((0.5,), (0.5,))  # normalization -> scale between -1 and 1
])

# load the MNIST dataset
dataset = datasets.MNIST(root="./data", train=True, transform=transform, download=True)

# load dataset in batches
dataLoader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# %% create Discriminator

class Discriminator(nn.Module):  # discriminator: attempts to determine if the images generated by the generator are real or fake
    
    def __init__(self):
        super(Discriminator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(image_size, 1024),  # input: image size, 1024: number of neurons, i.e., output of this layer
            nn.LeakyReLU(0.2),  # activation function with a slope of 0.2
            nn.Linear(1024, 512),  # from 1024 to 512 neurons
            nn.LeakyReLU(0.2),
            nn.Linear(512, 256),  # from 512 to 256
            nn.LeakyReLU(0.2),
            nn.Linear(256, 1),  # from 256 to a single output: real or fake
            nn.Sigmoid()  # scales the output between 0 and 1
        )
        
    def forward(self, img):
        return self.model(img.view(-1, image_size))  # flatten the image and feed it to the model

# %% create Generator

class Generator(nn.Module):  # generates images (28x28x1)
    
    def __init__(self, z_dim):
        super(Generator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(z_dim, 256),  # fully connected layer from input to 256 neurons
            nn.ReLU(),
            nn.Linear(256, 512),  # from 256 to 512 neurons
            nn.ReLU(),
            nn.Linear(512, 1024),
            nn.ReLU(),
            nn.Linear(1024, image_size),  # from 1024 to 784
            nn.Tanh()  # output activation function  
        )
    
    def forward(self, x):
        return self.model(x).view(-1, 1, 28, 28)  # reshape output to 28x28 image

# %% GAN training

# hyperparameters
learning_rate = 0.0002  # learning rate
z_dim = 100  # size of the random noise vector
epochs = 100  # number of training epochs

# initialize models: define generator and discriminator
generator = Generator(z_dim).to(device)
discriminator = Discriminator().to(device)

# define loss function and optimizers
criterion = nn.BCELoss()  # binary cross-entropy
g_optimizer = optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))  # generator optimizer
d_optimizer = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))  # discriminator optimizer

# start training loop
for epoch in range(epochs):
    for i, (real_imgs, _) in enumerate(dataLoader):  # load images
        real_imgs = real_imgs.to(device)
        batch_size = real_imgs.size(0)  # get the size of the current batch
        real_labels = torch.ones(batch_size, 1).to(device)  # label real images as 1
        fake_labels = torch.zeros(batch_size, 1).to(device)  # label fake images as 0
        1
        # train discriminator 
        z = torch.randn(batch_size, z_dim).to(device)  # generate random noise
        fake_imgs = generator(z)  # generate fake images using the generator
        real_loss = criterion(discriminator(real_imgs), real_labels)  # loss on real images
        fake_loss = criterion(discriminator(fake_imgs.detach()), fake_labels)  # loss on fake images
        d_loss = real_loss + fake_loss  # total discriminator loss
        
        d_optimizer.zero_grad()  # reset gradients
        d_loss.backward()  # backpropagation
        d_optimizer.step()  # update parameters
        
        # train generator
        g_loss = criterion(discriminator(fake_imgs), real_labels)  # generator loss
        g_optimizer.zero_grad()  # reset gradients
        g_loss.backward()  # backpropagation
        g_optimizer.step()  # update parameters
    
    print(f"Epoch {epoch + 1}/{epochs} d_loss: {d_loss.item():.3f}, g_loss: {g_loss.item():.3f}")

# %% model testing and performance evaluation

# generate images from random noise
with torch.no_grad():
    z = torch.randn(16, z_dim).to(device)  # generate 16 random noise vectors
    sample_imgs = generator(z).cpu()  # generate fake images with the generator
    grid = np.transpose(utils.make_grid(sample_imgs, nrow=4, normalize=True), (1, 2, 0))  # arrange images in a grid view
    plt.imshow(grid)
    plt.show()
# %%
