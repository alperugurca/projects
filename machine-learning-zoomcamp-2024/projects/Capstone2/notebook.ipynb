{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":88022,"databundleVersionId":10175212,"sourceType":"competition"}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ML Zoomcamp 2024 Competition\nLink: https://www.kaggle.com/competitions/ml-zoomcamp-2024-competition","metadata":{}},{"cell_type":"markdown","source":"# 1. All in One Cell","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport xgboost as xgb\nfrom datetime import datetime\n\n# Define data paths\nDATA_PATH = '/kaggle/input/ml-zoomcamp-2024-competition'\n\ndef load_and_check_data():\n    \"\"\"Load data and print basic information\"\"\"\n    # Load training data\n    sales_df = pd.read_csv(f'{DATA_PATH}/sales.csv')\n    \n    # Load and process test data\n    test_df = pd.read_csv(f'{DATA_PATH}/test.csv', sep=';')  # Use separator\n    \n    print(\"Sales DataFrame Info:\")\n    print(sales_df.info())\n    print(\"\\nSales DataFrame Head:\")\n    print(sales_df.head())\n    \n    print(\"\\nTest DataFrame Info:\")\n    print(test_df.info())\n    print(\"\\nTest DataFrame Head:\")\n    print(test_df.head())\n    \n    return sales_df, test_df\n\ndef prepare_features(df, is_train=True):\n    \"\"\"Create features from the available data\"\"\"\n    if is_train:\n        # For training data\n        df['date'] = pd.to_datetime(df['date'])\n    else:\n        # For test data\n        df['date'] = pd.to_datetime(df['date'], format='%d.%m.%Y')\n    \n    # Create date-based features\n    df['year'] = df['date'].dt.year\n    df['month'] = df['date'].dt.month\n    df['day'] = df['date'].dt.day\n    df['day_of_week'] = df['date'].dt.dayofweek\n    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n    \n    if is_train:\n        # Create lag features for training data\n        df['lag_1'] = df.groupby(['item_id', 'store_id'])['quantity'].shift(1)\n        df['lag_7'] = df.groupby(['item_id', 'store_id'])['quantity'].shift(7)\n        \n        # Create rolling mean features\n        df['rolling_mean_7'] = df.groupby(['item_id', 'store_id'])['quantity'].transform(\n            lambda x: x.rolling(window=7, min_periods=1).mean())\n        df['rolling_mean_30'] = df.groupby(['item_id', 'store_id'])['quantity'].transform(\n            lambda x: x.rolling(window=30, min_periods=1).mean())\n    else:\n        # For test data, initialize these columns with 0\n        df['lag_1'] = 0\n        df['lag_7'] = 0\n        df['rolling_mean_7'] = 0\n        df['rolling_mean_30'] = 0\n    \n    # Fill NaN values\n    df = df.fillna(0)\n    \n    return df\n\ndef prepare_training_data(df):\n    \"\"\"Prepare data for model training\"\"\"\n    # Select features for training\n    feature_columns = [\n        'year', 'month', 'day', 'day_of_week', 'is_weekend',\n        'lag_1', 'lag_7', 'rolling_mean_7', 'rolling_mean_30',\n        'store_id'  # Adding store_id as a feature\n    ]\n    \n    X = df[feature_columns]\n    y = df['quantity']\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    \n    return X_train, X_test, y_train, y_test, feature_columns\n\ndef train_model(X_train, y_train):\n    \"\"\"Train XGBoost model\"\"\"\n    model = xgb.XGBRegressor(\n        n_estimators=100,\n        learning_rate=0.1,\n        max_depth=7,\n        random_state=42\n    )\n    \n    model.fit(X_train, y_train)\n    return model\n\ndef evaluate_model(y_true, y_pred, model_name):\n    \"\"\"Evaluate model performance using multiple metrics\"\"\"\n    metrics = {\n        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n        'MAE': mean_absolute_error(y_true, y_pred),\n        'R2': r2_score(y_true, y_pred)\n    }\n    \n    print(f\"\\nModel: {model_name}\")\n    for metric, value in metrics.items():\n        print(f\"{metric}: {value:.4f}\")\n    \n    return metrics\n\ndef generate_submission(model, test_df, feature_columns):\n    \"\"\"Generate submission file\"\"\"\n    # Make predictions\n    test_features = test_df[feature_columns]\n    predictions = model.predict(test_features)\n    \n    # Create submission dataframe\n    submission = pd.DataFrame({\n        'row_id': test_df['row_id'],\n        'quantity': predictions\n    })\n    \n    # Save submission file\n    submission.to_csv('submission.csv', index=False)\n    print(\"Submission file created successfully!\")\n\ndef main():\n    # Load and check data\n    print(\"Loading and checking data...\")\n    train_df, test_df = load_and_check_data()\n    \n    # Prepare features\n    print(\"Preparing features...\")\n    train_df = prepare_features(train_df, is_train=True)\n    test_df = prepare_features(test_df, is_train=False)\n    \n    # Prepare training data\n    print(\"Preparing training data...\")\n    X_train, X_test, y_train, y_test, feature_columns = prepare_training_data(train_df)\n    \n    # Train model\n    print(\"Training model...\")\n    model = train_model(X_train, y_train)\n    \n    # Make predictions and evaluate\n    print(\"Evaluating model...\")\n    y_pred = model.predict(X_test)\n    metrics = evaluate_model(y_test, y_pred, \"XGBoost\")\n    \n    # Generate submission\n    print(\"Generating submission...\")\n    generate_submission(model, test_df, feature_columns)\n    \n    return model, metrics\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-20T18:02:25.109326Z","iopub.execute_input":"2025-01-20T18:02:25.109734Z","iopub.status.idle":"2025-01-20T18:03:43.737589Z","shell.execute_reply.started":"2025-01-20T18:02:25.109698Z","shell.execute_reply":"2025-01-20T18:03:43.736609Z"}},"outputs":[{"name":"stdout","text":"Loading and checking data...\nSales DataFrame Info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 7432685 entries, 0 to 7432684\nData columns (total 7 columns):\n #   Column      Dtype  \n---  ------      -----  \n 0   Unnamed: 0  int64  \n 1   date        object \n 2   item_id     object \n 3   quantity    float64\n 4   price_base  float64\n 5   sum_total   float64\n 6   store_id    int64  \ndtypes: float64(3), int64(2), object(2)\nmemory usage: 396.9+ MB\nNone\n\nSales DataFrame Head:\n   Unnamed: 0        date       item_id  quantity  price_base  sum_total  \\\n0           0  2023-08-04  293375605257     1.000       47.86      47.86   \n1           1  2023-08-04  a66fdf2c0ae7     3.000       49.60     148.80   \n2           2  2023-08-04  daa46ef49b7a     0.822      379.00     311.54   \n3           3  2023-08-04  a3b49c1bf758     1.000      129.00     129.00   \n4           4  2023-08-04  ab611c5cef62     7.000       79.90     559.30   \n\n   store_id  \n0         1  \n1         1  \n2         1  \n3         1  \n4         1  \n\nTest DataFrame Info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 883680 entries, 0 to 883679\nData columns (total 4 columns):\n #   Column    Non-Null Count   Dtype \n---  ------    --------------   ----- \n 0   row_id    883680 non-null  int64 \n 1   item_id   883680 non-null  object\n 2   store_id  883680 non-null  int64 \n 3   date      883680 non-null  object\ndtypes: int64(2), object(2)\nmemory usage: 27.0+ MB\nNone\n\nTest DataFrame Head:\n   row_id       item_id  store_id        date\n0       0  c578da8e8841         1  27.09.2024\n1       1  c578da8e8841         1  28.09.2024\n2       2  c578da8e8841         1  29.09.2024\n3       3  c578da8e8841         1  30.09.2024\n4       4  c578da8e8841         1  01.10.2024\nPreparing features...\nPreparing training data...\nTraining model...\nEvaluating model...\n\nModel: XGBoost\nRMSE: 20.6660\nMAE: 2.3716\nR2: 0.4580\nGenerating submission...\nSubmission file created successfully!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# 2. EDA","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\n# Define data paths\nDATA_PATH = '/kaggle/input/ml-zoomcamp-2024-competition'\n\n# Load the data first\nprint(\"Loading data...\")\ntrain_df = pd.read_csv(f'{DATA_PATH}/sales.csv')\n\n# Now perform EDA\ndef perform_eda(sales_df):\n    \"\"\"Perform Exploratory Data Analysis\"\"\"\n    print(\"=== Basic Statistics ===\")\n    print(\"\\nDescriptive Statistics for Numerical Columns:\")\n    print(sales_df.describe())\n    \n    print(\"\\n=== Missing Values Analysis ===\")\n    missing_values = sales_df.isnull().sum()\n    print(missing_values[missing_values > 0])\n    \n    print(\"\\n=== Target Variable Analysis ===\")\n    print(\"\\nQuantity Statistics:\")\n    print(f\"Mean quantity: {sales_df['quantity'].mean():.2f}\")\n    print(f\"Median quantity: {sales_df['quantity'].median():.2f}\")\n    print(f\"Min quantity: {sales_df['quantity'].min():.2f}\")\n    print(f\"Max quantity: {sales_df['quantity'].max():.2f}\")\n    \n    # Time-based analysis\n    sales_df['date'] = pd.to_datetime(sales_df['date'])\n    print(\"\\n=== Temporal Analysis ===\")\n    monthly_sales = sales_df.groupby(sales_df['date'].dt.to_period('M'))['quantity'].sum()\n    print(\"\\nMonthly Sales Trends:\")\n    print(monthly_sales)\n    \n    # Store and Item Analysis\n    print(\"\\n=== Store Analysis ===\")\n    store_stats = sales_df.groupby('store_id')['quantity'].agg(['mean', 'count'])\n    print(\"\\nStore-wise Statistics:\")\n    print(store_stats)\n    \n    print(\"\\n=== Item Analysis ===\")\n    item_stats = sales_df.groupby('item_id')['quantity'].agg(['mean', 'count'])\n    print(\"\\nTop 5 Items by Average Quantity:\")\n    print(item_stats.sort_values('mean', ascending=False).head())\n    \n    # Feature Correlations\n    numeric_cols = sales_df.select_dtypes(include=[np.number]).columns\n    correlations = sales_df[numeric_cols].corr()\n    print(\"\\n=== Feature Correlations ===\")\n    print(correlations['quantity'].sort_values(ascending=False))\n\n    return {\n        'monthly_sales': monthly_sales,\n        'store_stats': store_stats,\n        'item_stats': item_stats,\n        'correlations': correlations\n    }\n\n# Perform EDA\nprint(\"Performing EDA...\")\neda_results = perform_eda(train_df)","metadata":{"execution":{"iopub.status.busy":"2025-01-20T18:26:02.814191Z","iopub.execute_input":"2025-01-20T18:26:02.814584Z","iopub.status.idle":"2025-01-20T18:26:14.152615Z","shell.execute_reply.started":"2025-01-20T18:26:02.814537Z","shell.execute_reply":"2025-01-20T18:26:14.151678Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Loading data...\nPerforming EDA...\n=== Basic Statistics ===\n\nDescriptive Statistics for Numerical Columns:\n         Unnamed: 0      quantity    price_base     sum_total      store_id\ncount  7.432685e+06  7.432685e+06  7.432685e+06  7.432685e+06  7.432685e+06\nmean   1.235202e+07  5.642398e+00  2.075824e+02  7.612796e+02  2.041405e+00\nstd    7.625496e+06  2.740466e+01  3.372886e+02  4.789478e+03  1.206728e+00\nmin    0.000000e+00 -5.000000e+02 -2.167667e+04 -2.041080e+04  1.000000e+00\n25%    1.858171e+06  1.000000e+00  6.000000e+01  1.279000e+02  1.000000e+00\n50%    1.344877e+07  2.000000e+00  1.099000e+02  2.596000e+02  2.000000e+00\n75%    1.988142e+07  4.672000e+00  1.999000e+02  5.990000e+02  3.000000e+00\nmax    2.173959e+07  4.952000e+03  2.899990e+04  6.865242e+05  4.000000e+00\n\n=== Missing Values Analysis ===\nSeries([], dtype: int64)\n\n=== Target Variable Analysis ===\n\nQuantity Statistics:\nMean quantity: 5.64\nMedian quantity: 2.00\nMin quantity: -500.00\nMax quantity: 4952.00\n\n=== Temporal Analysis ===\n\nMonthly Sales Trends:\ndate\n2022-08     161935.849\n2022-09    1259823.091\n2022-10    1305737.196\n2022-11    1253136.177\n2022-12    1384980.873\n2023-01    1164877.864\n2023-02    1144071.437\n2023-03    1317607.818\n2023-04    1292487.185\n2023-05    1299462.496\n2023-06    1257164.310\n2023-07    1283329.160\n2023-08    1279795.523\n2023-09    1280345.459\n2023-10    1280006.881\n2023-11    1257515.496\n2023-12    2100388.999\n2024-01    1983563.822\n2024-02    2084869.321\n2024-03    2400666.942\n2024-04    2363726.246\n2024-05    2456781.689\n2024-06    2327192.568\n2024-07    2449740.145\n2024-08    2402887.389\n2024-09    2146071.269\nFreq: M, Name: quantity, dtype: float64\n\n=== Store Analysis ===\n\nStore-wise Statistics:\n              mean    count\nstore_id                   \n1         6.261101  3683750\n2         3.661610  1337889\n3         5.725016   830591\n4         5.833674  1580455\n\n=== Item Analysis ===\n\nTop 5 Items by Average Quantity:\n                     mean  count\nitem_id                         \n327c5bc1e583  1526.870000    100\nb0d24502fb66   888.722320   2276\n63161948a95a   570.681997    761\naba951017adf   296.263158    171\n9a7e315f3f42   280.718990   2007\n\n=== Feature Correlations ===\nquantity      1.000000\nsum_total     0.407415\nstore_id     -0.006533\nUnnamed: 0   -0.011266\nprice_base   -0.044354\nName: quantity, dtype: float64\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# 3. New Approaches","metadata":{}},{"cell_type":"code","source":"# Import all necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import Ridge\nimport xgboost as xgb\nfrom lightgbm import LGBMRegressor\nfrom datetime import datetime\n\n# Define data paths\nDATA_PATH = '/kaggle/input/ml-zoomcamp-2024-competition'\n\n# Load and prepare data\nprint(\"Loading and preparing data...\")\ntrain_df = pd.read_csv(f'{DATA_PATH}/sales.csv')\n\ndef prepare_features(df):\n    \"\"\"Create features from the available data\"\"\"\n    # Convert date and create date-based features\n    df['date'] = pd.to_datetime(df['date'])\n    df['year'] = df['date'].dt.year\n    df['month'] = df['date'].dt.month\n    df['day'] = df['date'].dt.day\n    df['day_of_week'] = df['date'].dt.dayofweek\n    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n    \n    # Create lag features\n    df['lag_1'] = df.groupby(['item_id', 'store_id'])['quantity'].shift(1)\n    df['lag_7'] = df.groupby(['item_id', 'store_id'])['quantity'].shift(7)\n    \n    # Create rolling mean features\n    df['rolling_mean_7'] = df.groupby(['item_id', 'store_id'])['quantity'].transform(\n        lambda x: x.rolling(window=7, min_periods=1).mean())\n    df['rolling_mean_30'] = df.groupby(['item_id', 'store_id'])['quantity'].transform(\n        lambda x: x.rolling(window=30, min_periods=1).mean())\n    \n    # Fill NaN values\n    df = df.fillna(0)\n    \n    return df\n\n# Prepare features\ntrain_df = prepare_features(train_df)\n\n# Select features for training\nfeature_columns = [\n    'year', 'month', 'day', 'day_of_week', 'is_weekend',\n    'lag_1', 'lag_7', 'rolling_mean_7', 'rolling_mean_30',\n    'store_id'\n]\n\n# Prepare X and y\nX = train_df[feature_columns]\ny = train_df['quantity']\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\ndef train_multiple_models(X_train, X_test, y_train, y_test):\n    \"\"\"Train and tune multiple models\"\"\"\n    models = {}\n    results = {}\n    \n    # 1. XGBoost with GridSearch\n    print(\"\\n=== Training XGBoost with GridSearch ===\")\n    xgb_params = {\n        'n_estimators': [100, 200],\n        'max_depth': [3, 5, 7],\n        'learning_rate': [0.01, 0.1],\n        'subsample': [0.8, 1.0]\n    }\n    xgb_model = xgb.XGBRegressor(random_state=42)\n    xgb_grid = GridSearchCV(xgb_model, xgb_params, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1)\n    xgb_grid.fit(X_train, y_train)\n    models['xgboost'] = xgb_grid.best_estimator_\n    results['xgboost'] = {\n        'best_params': xgb_grid.best_params_,\n        'best_score': -xgb_grid.best_score_\n    }\n    \n    # 2. LightGBM with GridSearch\n    print(\"\\n=== Training LightGBM with GridSearch ===\")\n    lgb_params = {\n        'n_estimators': [100, 200],\n        'num_leaves': [31, 63],\n        'learning_rate': [0.01, 0.1],\n        'subsample': [0.8, 1.0]\n    }\n    lgb_model = LGBMRegressor(random_state=42)\n    lgb_grid = GridSearchCV(lgb_model, lgb_params, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1)\n    lgb_grid.fit(X_train, y_train)\n    models['lightgbm'] = lgb_grid.best_estimator_\n    results['lightgbm'] = {\n        'best_params': lgb_grid.best_params_,\n        'best_score': -lgb_grid.best_score_\n    }\n    \n    # 3. Random Forest with GridSearch\n    print(\"\\n=== Training Random Forest with GridSearch ===\")\n    rf_params = {\n        'n_estimators': [100, 200],\n        'max_depth': [None, 10, 20],\n        'min_samples_split': [2, 5]\n    }\n    rf_model = RandomForestRegressor(random_state=42)\n    rf_grid = GridSearchCV(rf_model, rf_params, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1)\n    rf_grid.fit(X_train, y_train)\n    models['random_forest'] = rf_grid.best_estimator_\n    results['random_forest'] = {\n        'best_params': rf_grid.best_params_,\n        'best_score': -rf_grid.best_score_\n    }\n    \n    # 4. Ridge Regression\n    print(\"\\n=== Training Ridge Regression with GridSearch ===\")\n    ridge_params = {\n        'alpha': [0.1, 1.0, 10.0],\n        'solver': ['auto', 'svd']\n    }\n    ridge_model = Ridge(random_state=42)\n    ridge_grid = GridSearchCV(ridge_model, ridge_params, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1)\n    ridge_grid.fit(X_train, y_train)\n    models['ridge'] = ridge_grid.best_estimator_\n    results['ridge'] = {\n        'best_params': ridge_grid.best_params_,\n        'best_score': -ridge_grid.best_score_\n    }\n    \n    # Evaluate all models on test set\n    print(\"\\n=== Model Evaluation on Test Set ===\")\n    for name, model in models.items():\n        y_pred = model.predict(X_test)\n        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n        mae = mean_absolute_error(y_test, y_pred)\n        r2 = r2_score(y_test, y_pred)\n        \n        results[name].update({\n            'test_rmse': rmse,\n            'test_mae': mae,\n            'test_r2': r2\n        })\n        \n        print(f\"\\nModel: {name}\")\n        print(f\"Test RMSE: {rmse:.4f}\")\n        print(f\"Test MAE: {mae:.4f}\")\n        print(f\"Test R2: {r2:.4f}\")\n        print(f\"Best Parameters: {results[name]['best_params']}\")\n    \n    # Find best model\n    best_model = min(results.items(), key=lambda x: x[1]['test_rmse'])\n    print(f\"\\nBest Model: {best_model[0]} with RMSE: {best_model[1]['test_rmse']:.4f}\")\n    \n    return models, results\n\n# Train multiple models\nprint(\"Training multiple models with hyperparameter tuning...\")\nmodels, results = train_multiple_models(X_train, X_test, y_train, y_test)\n\n# Save the best model's results\nbest_model_name = min(results.items(), key=lambda x: x[1]['test_rmse'])[0]\nbest_model = models[best_model_name]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T20:28:07.936863Z","iopub.execute_input":"2025-01-20T20:28:07.937342Z","iopub.status.idle":"2025-01-20T20:28:07.942558Z","shell.execute_reply.started":"2025-01-20T20:28:07.937302Z","shell.execute_reply":"2025-01-20T20:28:07.941425Z"}},"outputs":[],"execution_count":16}]}